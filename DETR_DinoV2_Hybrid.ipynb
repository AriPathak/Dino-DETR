{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfmNvRdhLIs81w1TAsns3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AriPathak/Dino-DETR/blob/main/DETR_DinoV2_Hybrid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "g3jUUnz3t5RT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DinoV2Encoder(nn.Module):\n",
        "  def __init__(self, learnable_modules:list):\n",
        "    super().__init__()\n",
        "    model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
        "    self.learnable_modules = learnable_modules\n",
        "    #self.conv = nn.LazyConv2d(256, kernel_size=1)\n",
        "    self.ffn = nn.LazyLinear(256)\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "    for i in self.learnable_modules:\n",
        "      for param in model.blocks[i].parameters():\n",
        "        param.requires_grad = True\n",
        "    self.DinoV2 = nn.Sequential(model.patch_embed,\n",
        "                          *[model.blocks[p] for p in range(12)])\n",
        "  def forward(self, x):\n",
        "    x = self.DinoV2(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "baCjQqgyuCCc"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DETRDecoder(nn.Module):\n",
        "  def __init__(self, num_classes, decoder, hidden_dim=256, nheads=8,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6):\n",
        "    super().__init__()\n",
        "    self.transformer = decoder\n",
        "    self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
        "    self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
        "    self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.transpose(0, 1)\n",
        "    h = self.transformer(self.query_pos.unsqueeze(1), x).transpose(0, 1)\n",
        "    #first arg is input sequence to the encoder; second is input to the decoder\n",
        "    return {'pred_logits': self.linear_class(h),\n",
        "            'pred_boxes': self.linear_bbox(h).sigmoid()}"
      ],
      "metadata": {
        "id": "8B9UVd7EuAZM"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DETR(nn.Module):\n",
        "  def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
        "\n",
        "        # create a default PyTorch transformer\n",
        "    self.transformer = nn.Transformer(\n",
        "        hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
        "\n",
        "    # prediction heads, one extra class for predicting non-empty slots\n",
        "    # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
        "    self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
        "    self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "    # output positional encodings (object queries)\n",
        "    self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
        "\n",
        "    # spatial positional encodings\n",
        "    # note that in baseline DETR we use sine positional encodings\n",
        "    self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
        "    self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
        "\n",
        "  def forward(self, h):\n",
        "      # propagate inputs through ResNet-50 up to avg-pool layer\n",
        "\n",
        "      # convert from 2048 to 256 feature planes for the transformer\n",
        "      #[1, 2048, 7, 7]\n",
        "      #[1, 256, 7, 7]\n",
        "\n",
        "      # construct positional encodings\n",
        "      H, W = h.shape[-2:]\n",
        "      pos = torch.cat([\n",
        "          self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
        "          self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
        "      ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
        "\n",
        "        # propagate through the transformer\n",
        "\n",
        "      h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),\n",
        "                            self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
        "\n",
        "      #first arg is input sequence to the encoder; second is input to the decoder\n",
        "\n",
        "      # finally project transformer outputs to class labels and bounding boxes\n",
        "      return {'pred_logits': self.linear_class(h),\n",
        "              'pred_boxes': self.linear_bbox(h).sigmoid()}"
      ],
      "metadata": {
        "id": "zwaDnzVuuAbc"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detr_trans = DETR(91)\n",
        "state_dict = torch.hub.load_state_dict_from_url(\n",
        "    url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',\n",
        "    map_location='cpu', check_hash=True)\n",
        "detr_state_dict = state_dict.copy()\n",
        "for n, v in enumerate(state_dict):\n",
        "  if n >= 191 and not(456<=n<=457):\n",
        "    del detr_state_dict[v]\n",
        "\n",
        "detr_trans.load_state_dict(detr_state_dict)\n",
        "\n",
        "detr_decoder = detr_trans.transformer.decoder #money\n",
        "for c, param in detr_decoder.named_parameters():\n",
        "  param.requires_grad = False\n",
        "#TODO: unfreeze learnable modules for DETR Decoder\n",
        "#detr_decoder.eval();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcJPK-CkuAdb",
        "outputId": "9293903c-68f2-4c43-a7a7-a6cc9a4f69b5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DETR_DinoV2(nn.Module):\n",
        "  def __init__(self, learnable_modules, num_classes, decoder):\n",
        "    super().__init__()\n",
        "    self.decoder = DETRDecoder(num_classes, decoder)\n",
        "    self.encoder = DinoV2Encoder(learnable_modules)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    return self.decoder(x)"
      ],
      "metadata": {
        "id": "G3uWXFIMuS0t"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DETR_DinoV2_Hybrid = DETR_DinoV2([10, 11], 91, detr_decoder)\n",
        "test_img = torch.randn(1, 3, 672, 672)\n",
        "print(\"Predicted Classification Logits Output Shape: \" , list(DETR_DinoV2_Hybrid(test_img)['pred_logits'].shape))\n",
        "print(\"Predicted Bounding Box Output Shape: \" , list(DETR_DinoV2_Hybrid(test_img)['pred_boxes'].shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skvePzOguAhM",
        "outputId": "74b738aa-67bf-45d3-c107-4a9e3a372007"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pred Classification Logits Output Shape:  [1, 100, 92]\n",
            "Pred Bounding Box Output Shape:  [1, 100, 4]\n"
          ]
        }
      ]
    }
  ]
}